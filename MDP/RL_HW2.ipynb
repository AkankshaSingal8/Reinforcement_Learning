{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xNbgSZLmAr4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5\n",
        "Write code that solves the linear equations required to find vπ(s) and generate the values in the table in Figure 3.2. Note that the policy π picks all valid actions in a state with equal probability. Add comments to your code that explain all your steps."
      ],
      "metadata": {
        "id": "BqI6MjdAI8Sl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjgrtpTrIxqS"
      },
      "outputs": [],
      "source": [
        "action_probability = 1/4\n",
        "discount = 0.9\n",
        "\n",
        "initial_grid = np.random.random((5, 5))\n",
        "#states go from (0,0) to (4,4)\n",
        "\n",
        "# Special states A and B\n",
        "A = (0, 1)\n",
        "A_prime = (4, 1)\n",
        "B = (0, 3)\n",
        "B_prime = (2, 3)\n",
        "\n",
        "actions = ['U', 'D', 'L', 'R']\n",
        "\n",
        "grid_size = 5\n",
        "\n",
        "def next_state_and_reward(state, action):\n",
        "    row, col = state\n",
        "\n",
        "    if state == A:\n",
        "        return (A_prime, 10)  # Move to A_prime with a reward of +10\n",
        "    if state == B:\n",
        "        return (B_prime, 5)   # Move to B_prime with a reward of +5\n",
        "\n",
        "    elif action == 'U':\n",
        "        return (max(row - 1, 0), col), -(row == 0)\n",
        "    elif action == 'D':\n",
        "        return (min(row + 1, 4), col), -(row == 4)\n",
        "    elif action == 'L':\n",
        "        return (row, max(col - 1, 0)), -(col == 0)\n",
        "    elif action == 'R':\n",
        "        return (row, min(col + 1, 4)), -(col == 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Policy Evaluation\n",
        "def policy_evaluation(value_function, discount, action_probability):\n",
        "    theta=1e-4\n",
        "    for i in range(1000):\n",
        "        delta = 0\n",
        "\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                state = (i, j)\n",
        "                v = value_function[state]\n",
        "\n",
        "                new_value = 0\n",
        "                for action in actions:\n",
        "                    (next_state, reward) = next_state_and_reward(state, action)\n",
        "                    new_value += action_probability * (reward + discount * value_function[next_state])\n",
        "\n",
        "                value_function[state] = new_value\n",
        "\n",
        "                delta = max(delta, abs(v - new_value))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return value_function\n",
        "\n",
        "# Run policy evaluation\n",
        "value_function = policy_evaluation(value_function, discount, action_probability)\n",
        "\n",
        "# Display the value function\n",
        "print(\"State-Value Function:\")\n",
        "print(np.round(value_function, 1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpch5VSJJ59S",
        "outputId": "4fc080a5-f43e-49b7-90ac-9689daecbd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State-Value Function:\n",
            "[[ 3.3  8.8  4.4  5.3  1.5]\n",
            " [ 1.5  3.   2.3  1.9  0.5]\n",
            " [ 0.1  0.7  0.7  0.4 -0.4]\n",
            " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
            " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7\n",
        "Write code that generates the optimal state-value function and the optimal policy for the\n",
        "Gridworld in Figure 3.5. You want to solve the corresponding system of non-linear equations. Explain all your\n",
        "steps."
      ],
      "metadata": {
        "id": "A69x_lvAJOBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2CXywMJGHEg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_probability = 1/4\n",
        "discount = 0.9\n",
        "\n",
        "initial_grid = np.random.random((5, 5))\n",
        "#states go from (0,0) to (4,4)\n",
        "\n",
        "# Special states A and B\n",
        "A = (0, 1)\n",
        "A_prime = (4, 1)\n",
        "B = (0, 3)\n",
        "B_prime = (2, 3)\n",
        "\n",
        "actions = ['U', 'D', 'L', 'R']\n",
        "\n",
        "grid_size = 5\n",
        "\n",
        "def next_state_and_reward(state, action):\n",
        "    row, col = state\n",
        "\n",
        "    if state == A:\n",
        "        return (A_prime, 10)  # Move to A_prime with a reward of +10\n",
        "    if state == B:\n",
        "        return (B_prime, 5)   # Move to B_prime with a reward of +5\n",
        "\n",
        "    elif action == 'U':\n",
        "        return (max(row - 1, 0), col), -(row == 0)\n",
        "    elif action == 'D':\n",
        "        return (min(row + 1, 4), col), -(row == 4)\n",
        "    elif action == 'L':\n",
        "        return (row, max(col - 1, 0)), -(col == 0)\n",
        "    elif action == 'R':\n",
        "        return (row, min(col + 1, 4)), -(col == 4)\n"
      ],
      "metadata": {
        "id": "Lg3ph9aFJR0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Value Iteration algorithm to find optimal state-value function and policy\n",
        "def value_iteration(theta=1e-4, discount=0.9):\n",
        "    value_function = np.zeros((grid_size, grid_size))  # Initialize values to 0\n",
        "    policy = np.full((grid_size, grid_size), '', dtype=object)  # Initialize an empty policy\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        # Iterate over all states in the grid\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                state = (i, j)\n",
        "                v = value_function[state]\n",
        "\n",
        "                max_value = float('-inf')\n",
        "                best_actions = []\n",
        "                for action in actions:\n",
        "                    (next_state, reward) = next_state_and_reward(state, action)\n",
        "                    value = reward + discount * value_function[next_state]\n",
        "\n",
        "\n",
        "                    if value > max_value:\n",
        "                        max_value = value\n",
        "                        best_actions = [action]\n",
        "                    elif value == max_value:\n",
        "                        best_actions.append(action)\n",
        "\n",
        "                # Updating the state-value function and policy\n",
        "                value_function[state] = max_value\n",
        "                policy[state] = ''.join(best_actions)\n",
        "\n",
        "\n",
        "                delta = max(delta, abs(v - value_function[state]))\n",
        "\n",
        "        iteration += 1\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return value_function, policy\n",
        "\n",
        "\n",
        "optimal_value_function, optimal_policy = value_iteration()\n",
        "\n",
        "\n",
        "print(\"Optimal Value Function:\")\n",
        "print(np.round(optimal_value_function, 1))\n",
        "\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for row in optimal_policy:\n",
        "    print(' '.join(row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM_mJ-nKfGiD",
        "outputId": "8ab1f0df-0fac-4c80-ad5f-1b8a053ce494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[[22.  24.4 22.  19.4 17.5]\n",
            " [19.8 22.  19.8 17.8 16. ]\n",
            " [17.8 19.8 17.8 16.  14.4]\n",
            " [16.  17.8 16.  14.4 13. ]\n",
            " [14.4 16.  14.4 13.  11.7]]\n",
            "\n",
            "Optimal Policy:\n",
            "R UDLR L UDLR L\n",
            "UR U UL L L\n",
            "UR U UL UL UL\n",
            "UR U UL UL UL\n",
            "UR U UL UL UL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9.\n",
        "Code policy iteration and value iteration (VI) to solve the Gridworld in Example 4.1. Your code\n",
        "must log output of each iteration. Pick up a few sample iterations to show policy evaluation and improvement at\n",
        "work. Similarly, show using a few obtained iterations that every iteration of VI improves the value function. Your\n",
        "code must include the fix to the bug mentioned in Exercise 4.4."
      ],
      "metadata": {
        "id": "r7ipUIVgJU1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discount = 1.0\n",
        "\n",
        "initial_grid = np.random.random((4, 4))\n",
        "\n",
        "terminal_states = [(0, 0), (3, 3)]\n",
        "actions = ['U', 'D', 'L', 'R']\n",
        "reward = -1\n",
        "grid_size = 4\n",
        "theta = 1e-4\n",
        "# equiprobable random policy\n",
        "action_probability = 1/4\n",
        "\n",
        "# policy and value function\n",
        "policy = np.random.choice(actions, size=(grid_size, grid_size))\n",
        "value_function = np.zeros((grid_size, grid_size))\n",
        "\n",
        "def next_state_and_reward(state, action):\n",
        "    row, col = state\n",
        "\n",
        "\n",
        "    if action == 'U':\n",
        "        return (max(row - 1, 0), col), -1\n",
        "    elif action == 'D':\n",
        "        return (min(row + 1, 3), col), -1\n",
        "    elif action == 'L':\n",
        "        return (row, max(col - 1, 0)), -1\n",
        "    elif action == 'R':\n",
        "        return (row, min(col + 1, 3)), -1\n"
      ],
      "metadata": {
        "id": "xDiKTVH3IU97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Evaluation\n",
        "def policy_evaluation(policy, value_function):\n",
        "    for iter in range(100):\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                state = (i, j)\n",
        "                if state in terminal_states:\n",
        "                    continue  # Skip terminal states\n",
        "\n",
        "                v = value_function[state]\n",
        "\n",
        "                next_state, _ = next_state_and_reward(state, policy[state])\n",
        "                value_function[state] = reward + discount * value_function[next_state]\n",
        "\n",
        "    return value_function\n",
        "\n",
        "# Policy Improvement\n",
        "def policy_improvement(policy, value_function):\n",
        "    policy_stable = True\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            state = (i, j)\n",
        "            if state in terminal_states:\n",
        "                continue\n",
        "\n",
        "            old_action = policy[state]\n",
        "\n",
        "            action_values = []\n",
        "            for action in actions:\n",
        "                next_state, _ = next_state_and_reward(state, action)\n",
        "                action_values.append(reward + discount * value_function[next_state])\n",
        "\n",
        "\n",
        "            best_action = actions[np.argmax(action_values)]\n",
        "            policy[state] = best_action\n",
        "\n",
        "\n",
        "            if best_action != old_action:\n",
        "                policy_stable = False\n",
        "    return policy, policy_stable\n",
        "\n",
        "# Policy Iteration\n",
        "def policy_iteration():\n",
        "    # Initialize value function and random policy\n",
        "    value_function = np.zeros((grid_size, grid_size))\n",
        "    policy = np.random.choice(actions, size=(grid_size, grid_size))\n",
        "\n",
        "    for iteration in range(100):\n",
        "        print(f\"Policy Iteration {iteration}:\")\n",
        "        print(\"Policy:\")\n",
        "        print(policy)\n",
        "        print(\"Value Function:\")\n",
        "        print(value_function)\n",
        "\n",
        "        value_function = policy_evaluation(policy, value_function)\n",
        "\n",
        "        policy, policy_stable = policy_improvement(policy, value_function)\n",
        "\n",
        "        print(policy, value_function)\n",
        "\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return policy, value_function\n",
        "\n",
        "final_policy, final_value_function = policy_iteration()\n",
        "\n",
        "print(\"\\nFinal Policy:\")\n",
        "print(final_policy)\n",
        "print(\"\\nFinal Value Function:\")\n",
        "print(final_value_function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5HM1I4RRlcY",
        "outputId": "40e3bc71-18bd-41ba-eda5-71aca2328579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration 0:\n",
            "Policy:\n",
            "[['D' 'R' 'L' 'R']\n",
            " ['D' 'D' 'D' 'D']\n",
            " ['D' 'D' 'D' 'D']\n",
            " ['L' 'L' 'R' 'L']]\n",
            "Value Function:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[['D' 'L' 'D' 'D']\n",
            " ['U' 'R' 'D' 'D']\n",
            " ['U' 'R' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']] [[   0. -199. -200. -100.]\n",
            " [-100. -101.   -3.   -2.]\n",
            " [-100. -101.   -2.   -1.]\n",
            " [-100. -101.   -1.    0.]]\n",
            "Policy Iteration 1:\n",
            "Policy:\n",
            "[['D' 'L' 'D' 'D']\n",
            " ['U' 'R' 'D' 'D']\n",
            " ['U' 'R' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']]\n",
            "Value Function:\n",
            "[[   0. -199. -200. -100.]\n",
            " [-100. -101.   -3.   -2.]\n",
            " [-100. -101.   -2.   -1.]\n",
            " [-100. -101.   -1.    0.]]\n",
            "[['D' 'L' 'L' 'D']\n",
            " ['U' 'U' 'D' 'D']\n",
            " ['U' 'D' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']] [[ 0. -1. -4. -3.]\n",
            " [-1. -4. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "Policy Iteration 2:\n",
            "Policy:\n",
            "[['D' 'L' 'L' 'D']\n",
            " ['U' 'U' 'D' 'D']\n",
            " ['U' 'D' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']]\n",
            "Value Function:\n",
            "[[ 0. -1. -4. -3.]\n",
            " [-1. -4. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "[['D' 'L' 'L' 'D']\n",
            " ['U' 'U' 'U' 'D']\n",
            " ['U' 'U' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']] [[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "Policy Iteration 3:\n",
            "Policy:\n",
            "[['D' 'L' 'L' 'D']\n",
            " ['U' 'U' 'U' 'D']\n",
            " ['U' 'U' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']]\n",
            "Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "[['D' 'L' 'L' 'D']\n",
            " ['U' 'U' 'U' 'D']\n",
            " ['U' 'U' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']] [[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Final Policy:\n",
            "[['D' 'L' 'L' 'D']\n",
            " ['U' 'U' 'U' 'D']\n",
            " ['U' 'U' 'D' 'D']\n",
            " ['U' 'R' 'R' 'L']]\n",
            "\n",
            "Final Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value_function = np.zeros((grid_size, grid_size))\n",
        "policy = np.random.choice(actions, size=(grid_size, grid_size))\n",
        "\n",
        "# Value Iteration\n",
        "def value_iteration(value_function):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                state = (i, j)\n",
        "                if state in terminal_states:\n",
        "                    continue\n",
        "\n",
        "                v = value_function[state]\n",
        "\n",
        "                action_values = []\n",
        "                for action in actions:\n",
        "                    next_state, _ = next_state_and_reward(state, action)\n",
        "                    action_values.append(reward + discount * value_function[next_state])\n",
        "\n",
        "                value_function[state] = max(action_values)\n",
        "\n",
        "                delta = max(delta, abs(v - value_function[state]))\n",
        "\n",
        "        print(f\"Value function: {value_function}\")\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return value_function\n",
        "\n",
        "# Derive policy\n",
        "def derive_policy(value_function):\n",
        "    policy = np.zeros((grid_size, grid_size), dtype=str)\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            state = (i, j)\n",
        "            if state in terminal_states:\n",
        "                policy[state] = 'T'\n",
        "                continue\n",
        "\n",
        "            action_values = []\n",
        "            for action in actions:\n",
        "                next_state, _ = next_state_and_reward(state, action)\n",
        "                action_values.append(reward + discount * value_function[next_state])\n",
        "\n",
        "            best_action = actions[np.argmax(action_values)]\n",
        "            policy[state] = best_action\n",
        "\n",
        "    return policy\n",
        "\n",
        "final_value_function = value_iteration(value_function)\n",
        "\n",
        "optimal_policy = derive_policy(final_value_function)\n",
        "\n",
        "\n",
        "print(\"\\nFinal Value Function:\")\n",
        "print(final_value_function)\n",
        "print(\"\\nOptimal Policy:\")\n",
        "print(optimal_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXpGqPwAeIvO",
        "outputId": "e66d0a59-cdf7-4528-b6c4-36e843cb3065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Va;ue function: [[ 0. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1.  0.]]\n",
            "Va;ue function: [[ 0. -1. -2. -2.]\n",
            " [-1. -2. -2. -2.]\n",
            " [-2. -2. -2. -1.]\n",
            " [-2. -2. -1.  0.]]\n",
            "Va;ue function: [[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "Va;ue function: [[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Final Value Function:\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "Optimal Policy:\n",
            "[['T' 'L' 'L' 'D']\n",
            " ['U' 'U' 'U' 'D']\n",
            " ['U' 'U' 'D' 'D']\n",
            " ['U' 'R' 'R' 'T']]\n"
          ]
        }
      ]
    }
  ]
}